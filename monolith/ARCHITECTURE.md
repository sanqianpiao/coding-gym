# Monolith Architecture Summary

## Service Merger Completed âœ…

Successfully merged two independent microservices into a unified monolith:

```
BEFORE (Microservices)
â”œâ”€â”€ redis-token-bucket/     (Port 8081)
â”‚   â”œâ”€â”€ internal/bucket/
â”‚   â”œâ”€â”€ internal/handler/  
â”‚   â””â”€â”€ main.go
â””â”€â”€ outbox-kafka/          (Port 8080)
    â”œâ”€â”€ internal/config/
    â”œâ”€â”€ internal/database/
    â”œâ”€â”€ internal/kafka/
    â”œâ”€â”€ internal/models/
    â”œâ”€â”€ internal/relay/
    â”œâ”€â”€ internal/service/
    â””â”€â”€ cmd/server/

AFTER (Monolith)
â””â”€â”€ monolith/              (Port 8080)
    â”œâ”€â”€ internal/
    â”‚   â”œâ”€â”€ bucket/        â† Token bucket logic
    â”‚   â”œâ”€â”€ handler/       â† Token bucket HTTP handlers
    â”‚   â”œâ”€â”€ config/        â† Configuration management
    â”‚   â”œâ”€â”€ database/      â† Database layer
    â”‚   â”œâ”€â”€ kafka/         â† Kafka producer
    â”‚   â”œâ”€â”€ models/        â† Data models
    â”‚   â”œâ”€â”€ relay/         â† Outbox relay service
    â”‚   â””â”€â”€ service/       â† User business logic
    â””â”€â”€ cmd/
        â”œâ”€â”€ unified-server/ â† Single HTTP server
        â”œâ”€â”€ relay/         â† Background service
        â”œâ”€â”€ migrate/       â† Database migrations
        â””â”€â”€ backfill/      â† Event replay tool
```

## API Unification

### Single Entry Point: `http://localhost:8080`

#### Token Bucket API (Rate Limiting)
- `GET /api/bucket/check?key={key}` - Check available tokens
- `POST /api/bucket/consume?key={key}&tokens={n}` - Consume tokens
- `POST /api/bucket/reset` - Reset bucket state
- `POST /api/bucket/bulk-consume` - Bulk consume tokens

#### User Management API (Outbox Pattern)
- `POST /api/users` - Create user (triggers outbox event)
- `GET /api/users/{id}` - Get user by ID
- `PUT /api/users/{id}` - Update user (triggers outbox event)

#### Health & Testing
- `GET /health` - Overall health
- `GET /health/bucket` - Token bucket health
- `GET /health/users` - User service health
- `POST /api/test/crash` - Crash simulation

## Infrastructure Consolidation

### Single Docker Compose
```yaml
services:
  postgres:    # User data + outbox events
  redis:       # Token bucket state
  kafka:       # Event streaming
  zookeeper:   # Kafka dependency
  kafka-ui:    # Monitoring (optional)
```

### Unified Dependencies
- **From redis-token-bucket**: Redis client, Gorilla Mux
- **From outbox-kafka**: PostgreSQL, Kafka, UUID, Migrations
- **Combined**: Single go.mod with all dependencies

## Key Benefits Achieved

1. **Single Deployment Unit**: One binary instead of two
2. **Shared Infrastructure**: Reduced Docker containers and resource usage
3. **Unified API**: Single endpoint for clients to consume
4. **Simplified Operations**: One Makefile, one config, one monitoring stack
5. **Preserved Boundaries**: Clean separation via API prefixes
6. **Easy Migration Path**: Can extract back to microservices if needed

## Preserved Service Characteristics

### Token Bucket Service
- âœ… Redis-backed rate limiting
- âœ… Sliding window algorithm
- âœ… Atomic operations via Lua scripts
- âœ… Bulk consumption support
- âœ… Per-key isolation

### Outbox-Kafka Service  
- âœ… Exactly-once delivery guarantee
- âœ… Transactional outbox pattern
- âœ… Background relay processing
- âœ… Crash recovery capabilities
- âœ… Partition routing strategies
- âœ… Backfill/replay tooling

## Migration Verification

âœ… **Build System**: All binaries compile successfully
âœ… **Import Paths**: Updated to use monolith module  
âœ… **API Routes**: Both services accessible via prefixed endpoints
âœ… **Configuration**: Environment-based config for all services
âœ… **Infrastructure**: Combined Docker Compose with all dependencies
âœ… **Documentation**: Comprehensive README with usage examples
âœ… **Operations**: Full Makefile with development and production commands

The monolith successfully combines the best of both services while maintaining clean boundaries and operational simplicity.

## ğŸ”„ Relay Service Deep Dive

The relay service is the **heart of the outbox pattern** that ensures exactly-once delivery from the database to Kafka. It's a critical component that guarantees message reliability even in the face of system failures.

### ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User API      â”‚    â”‚   Relay Service  â”‚    â”‚     Kafka       â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ 1. Create User  â”‚    â”‚ 2. Poll Outbox   â”‚    â”‚ 3. Publish      â”‚
â”‚ 2. Insert Outboxâ”‚â”€â”€â”€â”€â”‚ 3. Process Eventsâ”œâ”€â”€â”€â”€â”‚    Events       â”‚
â”‚    (Transaction)â”‚    â”‚ 4. Mark as SENT  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Relay Workflow

#### **1. Continuous Polling Loop**
```go
// Polls every 5 seconds (configurable via RELAY_POLL_INTERVAL)
ticker := time.NewTicker(time.Duration(r.config.PollInterval) * time.Second)

// Queries for pending events
SELECT id, aggregate_type, aggregate_id, event_type, event_data,
       status, topic, partition_key, created_at, processed_at,
       retry_count, max_retries, error_message
FROM outbox_events 
WHERE status = 'NEW' 
ORDER BY created_at ASC 
LIMIT 100  -- Configurable batch size
```

#### **2. Event Processing Pipeline**
For each batch of pending events, the relay executes this pipeline:

```go
For each event in batch:
â”œâ”€ 1. Validate retry count (max 3 attempts by default)
â”œâ”€ 2. Begin database transaction
â”œâ”€ 3. Mark event as 'PROCESSING' (prevents concurrent processing)
â”œâ”€ 4. Commit transaction (atomic status change)
â”œâ”€ 5. Publish event to Kafka
â”œâ”€ 6. If Kafka success: Mark as 'SENT'
â””â”€ 7. If Kafka failure: Mark as 'FAILED', increment retry_count
```

#### **3. Atomic State Transitions**
The relay uses database transactions to ensure atomic state changes:

```go
// Step 1: Atomic status transition to PROCESSING
tx.Begin()
UPDATE outbox_events 
SET status = 'PROCESSING', processed_at = NOW() 
WHERE id = ? AND status = 'NEW'
tx.Commit()

// Step 2: After successful Kafka publish
tx.Begin()
UPDATE outbox_events 
SET status = 'SENT', processed_at = NOW() 
WHERE id = ?
tx.Commit()
```

### ğŸ›¡ï¸ Fault Tolerance Features

#### **1. Crash Recovery**
On startup, the relay automatically recovers from crashes:

```go
// Reset events that were interrupted during processing
func ResetStaleProcessingEvents(timeout time.Duration) {
    UPDATE outbox_events 
    SET status = 'NEW', error_message = 'Reset after processing timeout'
    WHERE status = 'PROCESSING' 
    AND processed_at < NOW() - interval '5 minutes'
}
```

**Why this works**: Events stuck in `PROCESSING` state indicate the previous relay instance crashed before completing the Kafka publish.

#### **2. Retry Logic with Exponential Backoff**
```go
if event.RetryCount >= event.MaxRetries {
    // Mark as permanently failed after max attempts (default: 3)
    MarkEventAsFailed(event, "exceeded maximum retry attempts")
    log.Printf("Event %s permanently failed after %d attempts", 
               event.ID, event.MaxRetries)
} else {
    // Increment retry count and let next polling cycle retry
    event.RetryCount++
    MarkEventAsFailed(event, kafkaError.Error())
}
```

#### **3. Concurrent Safety**
Multiple relay instances can run safely without conflicts:

- **Row-level locking**: `UPDATE WHERE status = 'NEW'` prevents race conditions
- **Atomic updates**: Status transitions are transactional
- **Idempotent operations**: Same event won't be published twice to Kafka

### ğŸ“Š Event State Machine

```
     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
     â”‚ NEW â”‚â”€â”€â”€â–¶â”‚ PROCESSING â”‚â”€â”€â”€â–¶â”‚ SENT â”‚  âœ… Success Path
     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜
        â”‚            â”‚
        â”‚            â–¼
        â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â–¶â”‚  FAILED  â”‚  âŒ After max retries
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ NEW (reset) â”‚  ğŸ”„ Crash recovery
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš™ï¸ Configuration Parameters

```bash
# Relay Service Configuration
RELAY_POLL_INTERVAL=5     # Poll every 5 seconds
RELAY_BATCH_SIZE=100      # Process 100 events per batch
RELAY_MAX_RETRIES=3       # Retry failed events 3 times
RELAY_PROCESSING_TIMEOUT=300  # Reset stale events after 5 minutes
```

### ğŸš€ Running the Relay Service

```bash
# Start as separate background service
make relay

# Or run directly
go run ./cmd/relay

# Check relay status
curl http://localhost:8080/health/relay
```

### ğŸ“ˆ Exactly-Once Delivery Guarantees

The relay ensures **exactly-once delivery** through multiple mechanisms:

1. **Transactional Outbox**: User data + outbox event created in single database transaction
   ```sql
   BEGIN;
   INSERT INTO users (id, email, name) VALUES (...);
   INSERT INTO outbox_events (id, event_type, event_data) VALUES (...);
   COMMIT;
   ```

2. **Atomic Processing**: Status changes prevent duplicate processing
   ```sql
   -- Only one relay instance can claim an event
   UPDATE outbox_events SET status = 'PROCESSING' 
   WHERE id = ? AND status = 'NEW'
   ```

3. **Idempotent Kafka Producer**: Kafka's idempotent producer prevents duplicates
   ```go
   producerConfig.Idempotent = true
   producerConfig.RequiredAcks = sarama.WaitForAll
   ```

4. **Crash Recovery**: Interrupted events are automatically reprocessed
5. **Retry Logic**: Transient failures (network, Kafka unavailable) are handled gracefully

### ğŸ” Monitoring & Observability

#### **Database Queries for Monitoring**
```sql
-- Check event status distribution
SELECT status, COUNT(*) as count 
FROM outbox_events 
GROUP BY status;

-- Events pending processing
SELECT COUNT(*) as pending_events 
FROM outbox_events 
WHERE status = 'NEW';

-- Failed events requiring attention
SELECT id, event_type, retry_count, error_message, created_at
FROM outbox_events 
WHERE status = 'FAILED' 
ORDER BY created_at DESC;
```

#### **Operational Commands**
```bash
# Monitor outbox table
make check-outbox

# View relay logs
make logs-relay

# Check relay health
curl http://localhost:8080/health/relay
```

### ğŸ¯ Why This Design Works

1. **Reliability**: No events are lost even if the relay crashes mid-processing
2. **Performance**: Batch processing reduces database load
3. **Scalability**: Multiple relay instances can run concurrently
4. **Debuggability**: Full audit trail of event processing in database
5. **Simplicity**: Single responsibility - poll database, publish to Kafka

The relay service transforms the potentially unreliable operation of "publish to Kafka" into a reliable, exactly-once delivery system by leveraging the database as a durable queue with ACID guarantees.